# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""
æ¨¡å‹è®­ç»ƒæ¨¡å—

è¯¥æ¨¡å—æä¾›åœ¨æ•°æ®é›†ä¸Šè®­ç»ƒ YOLO æ¨¡å‹çš„åŠŸèƒ½ã€‚
æ”¯æŒå• GPU å’Œå¤š GPU åˆ†å¸ƒå¼è®­ç»ƒï¼ŒåŒ…æ‹¬è‡ªåŠ¨æ··åˆç²¾åº¦ã€æ—©åœã€æ¨¡å‹ EMA ç­‰ç‰¹æ€§ã€‚

ä½¿ç”¨ç¤ºä¾‹:
    $ yolo mode=train model=yolo11n.pt data=coco8.yaml imgsz=640 epochs=100 batch=16
"""

from __future__ import annotations  # å¯ç”¨å»¶è¿Ÿç±»å‹æ³¨è§£è¯„ä¼°

# æ ‡å‡†åº“å¯¼å…¥
import gc  # åƒåœ¾å›æ”¶
import math  # æ•°å­¦å‡½æ•°
import os  # æ“ä½œç³»ç»Ÿæ¥å£
import subprocess  # å­è¿›ç¨‹ç®¡ç†
import time  # æ—¶é—´ç›¸å…³å‡½æ•°
import warnings  # è­¦å‘Šæ§åˆ¶
from copy import copy, deepcopy  # å¯¹è±¡æ‹·è´
from datetime import datetime, timedelta  # æ—¥æœŸæ—¶é—´å¤„ç†
from pathlib import Path  # è·¨å¹³å°è·¯å¾„æ“ä½œ

# ç¬¬ä¸‰æ–¹åº“å¯¼å…¥
import numpy as np  # æ•°ç»„å’Œæ•°å€¼è®¡ç®—
import torch  # PyTorch æ·±åº¦å­¦ä¹ æ¡†æ¶
from torch import distributed as dist  # åˆ†å¸ƒå¼è®­ç»ƒ
from torch import nn, optim  # ç¥ç»ç½‘ç»œå’Œä¼˜åŒ–å™¨

# Ultralytics æ¨¡å—å¯¼å…¥
from ultralytics import __version__  # ç‰ˆæœ¬å·
from ultralytics.cfg import get_cfg, get_save_dir  # é…ç½®ç®¡ç†
from ultralytics.data.utils import check_cls_dataset, check_det_dataset  # æ•°æ®é›†æ£€æŸ¥
from ultralytics.nn.tasks import load_checkpoint  # æ£€æŸ¥ç‚¹åŠ è½½
from ultralytics.utils import (  # å·¥å…·å‡½æ•°
    DEFAULT_CFG,  # é»˜è®¤é…ç½®
    GIT,  # Git ä¿¡æ¯
    LOCAL_RANK,  # æœ¬åœ°è¿›ç¨‹ç¼–å·
    LOGGER,  # æ—¥å¿—è®°å½•å™¨
    RANK,  # å…¨å±€è¿›ç¨‹ç¼–å·
    TQDM,  # è¿›åº¦æ¡
    YAML,  # YAML å¤„ç†
    callbacks,  # å›è°ƒå‡½æ•°
    clean_url,  # æ¸…ç† URL
    colorstr,  # å½©è‰²å­—ç¬¦ä¸²
    emojis,  # è¡¨æƒ…ç¬¦å·
)
from ultralytics.utils.autobatch import check_train_batch_size  # è‡ªåŠ¨æ‰¹é‡å¤§å°æ£€æŸ¥
from ultralytics.utils.checks import check_amp, check_file, check_imgsz, check_model_file_from_stem, print_args  # æ£€æŸ¥å‡½æ•°
from ultralytics.utils.dist import ddp_cleanup, generate_ddp_command  # åˆ†å¸ƒå¼è®­ç»ƒå·¥å…·
from ultralytics.utils.files import get_latest_run  # æ–‡ä»¶å·¥å…·
from ultralytics.utils.plotting import plot_results  # ç»“æœç»˜å›¾
from ultralytics.utils.torch_utils import (  # PyTorch å·¥å…·å‡½æ•°
    TORCH_2_4,  # PyTorch 2.4 ç‰ˆæœ¬æ ‡å¿—
    EarlyStopping,  # æ—©åœæœºåˆ¶
    ModelEMA,  # æ¨¡å‹æŒ‡æ•°ç§»åŠ¨å¹³å‡
    attempt_compile,  # å°è¯•ç¼–è¯‘æ¨¡å‹
    autocast,  # è‡ªåŠ¨æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
    convert_optimizer_state_dict_to_fp16,  # ä¼˜åŒ–å™¨çŠ¶æ€è½¬ FP16
    init_seeds,  # åˆå§‹åŒ–éšæœºç§å­
    one_cycle,  # OneCycle å­¦ä¹ ç‡è°ƒåº¦
    select_device,  # é€‰æ‹©è®¾å¤‡
    strip_optimizer,  # ç²¾ç®€ä¼˜åŒ–å™¨
    torch_distributed_zero_first,  # åˆ†å¸ƒå¼åŒæ­¥ä¸Šä¸‹æ–‡
    unset_deterministic,  # å–æ¶ˆç¡®å®šæ€§è®¾ç½®
    unwrap_model,  # è§£åŒ…æ¨¡å‹
)


class BaseTrainer:
    """ç”¨äºåˆ›å»ºè®­ç»ƒå™¨çš„åŸºç±»ã€‚

    è¯¥ç±»ä¸ºè®­ç»ƒ YOLO æ¨¡å‹æä¾›äº†åŸºç¡€æ¡†æ¶,å¤„ç†è®­ç»ƒå¾ªç¯ã€éªŒè¯ã€æ£€æŸ¥ç‚¹ä¿å­˜ä»¥åŠå„ç§è®­ç»ƒå®ç”¨å·¥å…·ã€‚
    æ”¯æŒå• GPU å’Œå¤š GPU åˆ†å¸ƒå¼è®­ç»ƒã€‚

    å±æ€§:
        args (SimpleNamespace): è®­ç»ƒå™¨çš„é…ç½®å‚æ•°ã€‚
        validator (BaseValidator): éªŒè¯å™¨å®ä¾‹ã€‚
        model (nn.Module): æ¨¡å‹å®ä¾‹ã€‚
        callbacks (defaultdict): å›è°ƒå‡½æ•°å­—å…¸ã€‚
        save_dir (Path): ä¿å­˜ç»“æœçš„ç›®å½•ã€‚
        wdir (Path): ä¿å­˜æƒé‡çš„ç›®å½•ã€‚
        last (Path): æœ€æ–°æ£€æŸ¥ç‚¹çš„è·¯å¾„ã€‚
        best (Path): æœ€ä½³æ£€æŸ¥ç‚¹çš„è·¯å¾„ã€‚
        save_period (int): æ¯ x ä¸ª epoch ä¿å­˜æ£€æŸ¥ç‚¹(< 1 æ—¶ç¦ç”¨)ã€‚
        batch_size (int): è®­ç»ƒçš„æ‰¹æ¬¡å¤§å°ã€‚
        epochs (int): è®­ç»ƒçš„ epoch æ•°é‡ã€‚
        start_epoch (int): è®­ç»ƒçš„èµ·å§‹ epochã€‚
        device (torch.device): ç”¨äºè®­ç»ƒçš„è®¾å¤‡ã€‚
        amp (bool): å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦(AMP)çš„æ ‡å¿—ã€‚
        scaler (amp.GradScaler): AMP çš„æ¢¯åº¦ç¼©æ”¾å™¨ã€‚
        data (str): æ•°æ®è·¯å¾„ã€‚
        ema (nn.Module): æ¨¡å‹çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡(EMA)ã€‚
        resume (bool): ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒã€‚
        lf (nn.Module): æŸå¤±å‡½æ•°ã€‚
        scheduler (torch.optim.lr_scheduler._LRScheduler): å­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚
        best_fitness (float): å·²è¾¾åˆ°çš„æœ€ä½³é€‚åº”åº¦å€¼ã€‚
        fitness (float): å½“å‰é€‚åº”åº¦å€¼ã€‚
        loss (float): å½“å‰æŸå¤±å€¼ã€‚
        tloss (float): æ€»æŸå¤±å€¼ã€‚
        loss_names (list): æŸå¤±åç§°åˆ—è¡¨ã€‚
        csv (Path): ç»“æœ CSV æ–‡ä»¶è·¯å¾„ã€‚
        metrics (dict): æŒ‡æ ‡å­—å…¸ã€‚
        plots (dict): ç»˜å›¾å­—å…¸ã€‚

    æ–¹æ³•:
        train: æ‰§è¡Œè®­ç»ƒè¿‡ç¨‹ã€‚
        validate: åœ¨æµ‹è¯•é›†ä¸Šè¿è¡ŒéªŒè¯ã€‚
        save_model: ä¿å­˜æ¨¡å‹è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚
        get_dataset: è·å–è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€‚
        setup_model: åŠ è½½ã€åˆ›å»ºæˆ–ä¸‹è½½æ¨¡å‹ã€‚
        build_optimizer: ä¸ºæ¨¡å‹æ„å»ºä¼˜åŒ–å™¨ã€‚

    ç¤ºä¾‹:
        åˆå§‹åŒ–è®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
        >>> trainer = BaseTrainer(cfg="config.yaml")
        >>> trainer.train()
    """

    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        """åˆå§‹åŒ– BaseTrainer ç±»ã€‚

        å‚æ•°:
            cfg (str, optional): é…ç½®æ–‡ä»¶è·¯å¾„ã€‚
            overrides (dict, optional): é…ç½®è¦†ç›–å‚æ•°ã€‚
            _callbacks (list, optional): å›è°ƒå‡½æ•°åˆ—è¡¨ã€‚
        """
        self.hub_session = overrides.pop("session", None)  # HUB
        self.args = get_cfg(cfg, overrides)
        self.check_resume(overrides)
        self.device = select_device(self.args.device)
        # Update "-1" devices so post-training val does not repeat search
        self.args.device = os.getenv("CUDA_VISIBLE_DEVICES") if "cuda" in str(self.device) else str(self.device)
        self.validator = None
        self.metrics = None
        self.plots = {}
        init_seeds(self.args.seed + 1 + RANK, deterministic=self.args.deterministic)

        # Dirs
        self.save_dir = get_save_dir(self.args)
        self.args.name = self.save_dir.name  # update name for loggers
        self.wdir = self.save_dir / "weights"  # weights dir
        if RANK in {-1, 0}:
            self.wdir.mkdir(parents=True, exist_ok=True)  # make dir
            self.args.save_dir = str(self.save_dir)
            # Save run args, serializing augmentations as reprs for resume compatibility
            args_dict = vars(self.args).copy()
            if args_dict.get("augmentations") is not None:
                # Serialize Albumentations transforms as their repr strings for checkpoint compatibility
                args_dict["augmentations"] = [repr(t) for t in args_dict["augmentations"]]
            YAML.save(self.save_dir / "args.yaml", args_dict)  # save run args
        self.last, self.best = self.wdir / "last.pt", self.wdir / "best.pt"  # checkpoint paths
        self.save_period = self.args.save_period

        self.batch_size = self.args.batch
        self.epochs = self.args.epochs or 100  # in case users accidentally pass epochs=None with timed training
        self.start_epoch = 0
        if RANK == -1:
            print_args(vars(self.args))

        # Device
        if self.device.type in {"cpu", "mps"}:
            self.args.workers = 0  # faster CPU training as time dominated by inference, not dataloading

        # Model and Dataset
        self.model = check_model_file_from_stem(self.args.model)  # add suffix, i.e. yolo11n -> yolo11n.pt
        with torch_distributed_zero_first(LOCAL_RANK):  # avoid auto-downloading dataset multiple times
            self.data = self.get_dataset()

        self.ema = None

        # Optimization utils init
        self.lf = None
        self.scheduler = None

        # Epoch level metrics
        self.best_fitness = None
        self.fitness = None
        self.loss = None
        self.tloss = None
        self.loss_names = ["Loss"]
        self.csv = self.save_dir / "results.csv"
        if self.csv.exists() and not self.args.resume:
            self.csv.unlink()
        self.plot_idx = [0, 1, 2]
        self.nan_recovery_attempts = 0

        # Callbacks
        self.callbacks = _callbacks or callbacks.get_default_callbacks()

        if isinstance(self.args.device, str) and len(self.args.device):  # i.e. device='0' or device='0,1,2,3'
            world_size = len(self.args.device.split(","))
        elif isinstance(self.args.device, (tuple, list)):  # i.e. device=[0, 1, 2, 3] (multi-GPU from CLI is list)
            world_size = len(self.args.device)
        elif self.args.device in {"cpu", "mps"}:  # i.e. device='cpu' or 'mps'
            world_size = 0
        elif torch.cuda.is_available():  # i.e. device=None or device='' or device=number
            world_size = 1  # default to device 0
        else:  # i.e. device=None or device=''
            world_size = 0

        self.ddp = world_size > 1 and "LOCAL_RANK" not in os.environ
        self.world_size = world_size
        # Run subprocess if DDP training, else train normally
        if RANK in {-1, 0} and not self.ddp:
            callbacks.add_integration_callbacks(self)
            # Start console logging immediately at trainer initialization
            self.run_callbacks("on_pretrain_routine_start")

    def add_callback(self, event: str, callback):
        """å°†ç»™å®šçš„å›è°ƒå‡½æ•°æ·»åŠ åˆ°äº‹ä»¶çš„å›è°ƒåˆ—è¡¨ä¸­ã€‚"""
        self.callbacks[event].append(callback)

    def set_callback(self, event: str, callback):
        """ç”¨ç»™å®šçš„å›è°ƒå‡½æ•°è¦†ç›–æŒ‡å®šäº‹ä»¶çš„ç°æœ‰å›è°ƒã€‚"""
        self.callbacks[event] = [callback]

    def run_callbacks(self, event: str):
        """è¿è¡Œä¸ç‰¹å®šäº‹ä»¶å…³è”çš„æ‰€æœ‰ç°æœ‰å›è°ƒã€‚"""
        for callback in self.callbacks.get(event, []):
            callback(self)

    def train(self):
        """åœ¨å¤š GPU ç³»ç»Ÿä¸Šå…è®¸ device='' æˆ– device=None é»˜è®¤ä¸º device=0ã€‚"""
        # Run subprocess if DDP training, else train normally
        if self.ddp:
            # Argument checks
            if self.args.rect:
                LOGGER.warning("'rect=True' is incompatible with Multi-GPU training, setting 'rect=False'")
                self.args.rect = False
            if self.args.batch < 1.0:
                raise ValueError(
                    "AutoBatch with batch<1 not supported for Multi-GPU training, "
                    f"please specify a valid batch size multiple of GPU count {self.world_size}, i.e. batch={self.world_size * 8}."
                )

            # Command
            cmd, file = generate_ddp_command(self)
            try:
                LOGGER.info(f"{colorstr('DDP:')} debug command {' '.join(cmd)}")
                subprocess.run(cmd, check=True)
            except Exception as e:
                raise e
            finally:
                ddp_cleanup(self, str(file))

        else:
            self._do_train()

    def _setup_scheduler(self):
        """åˆå§‹åŒ–è®­ç»ƒå­¦ä¹ ç‡è°ƒåº¦å™¨ã€‚"""
        if self.args.cos_lr:
            self.lf = one_cycle(1, self.args.lrf, self.epochs)  # cosine 1->hyp['lrf']
        else:
            self.lf = lambda x: max(1 - x / self.epochs, 0) * (1.0 - self.args.lrf) + self.args.lrf  # linear
        self.scheduler = optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda=self.lf)

    def _setup_ddp(self):
        """åˆå§‹åŒ–å¹¶è®¾ç½®åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ(DDP)è®­ç»ƒå‚æ•°ã€‚"""
        torch.cuda.set_device(RANK)
        self.device = torch.device("cuda", RANK)
        os.environ["TORCH_NCCL_BLOCKING_WAIT"] = "1"  # set to enforce timeout
        dist.init_process_group(
            backend="nccl" if dist.is_nccl_available() else "gloo",
            timeout=timedelta(seconds=10800),  # 3 hours
            rank=RANK,
            world_size=self.world_size,
        )

    def _setup_train(self):
        """åœ¨æ­£ç¡®çš„ rank è¿›ç¨‹ä¸Šæ„å»ºæ•°æ®åŠ è½½å™¨å’Œä¼˜åŒ–å™¨ã€‚"""
        ckpt = self.setup_model()
        self.model = self.model.to(self.device)
        self.set_model_attributes()

        # Compile model
        self.model = attempt_compile(self.model, device=self.device, mode=self.args.compile)

        # Freeze layers
        freeze_list = (
            self.args.freeze
            if isinstance(self.args.freeze, list)
            else range(self.args.freeze)
            if isinstance(self.args.freeze, int)
            else []
        )
        always_freeze_names = [".dfl"]  # always freeze these layers
        freeze_layer_names = [f"model.{x}." for x in freeze_list] + always_freeze_names
        self.freeze_layer_names = freeze_layer_names
        for k, v in self.model.named_parameters():
            # v.register_hook(lambda x: torch.nan_to_num(x))  # NaN to 0 (commented for erratic training results)
            if any(x in k for x in freeze_layer_names):
                LOGGER.info(f"Freezing layer '{k}'")
                v.requires_grad = False
            elif not v.requires_grad and v.dtype.is_floating_point:  # only floating point Tensor can require gradients
                LOGGER.warning(
                    f"setting 'requires_grad=True' for frozen layer '{k}'. "
                    "See ultralytics.engine.trainer for customization of frozen layers."
                )
                v.requires_grad = True

        # Check AMP
        self.amp = torch.tensor(self.args.amp).to(self.device)  # True or False
        if self.amp and RANK in {-1, 0}:  # Single-GPU and DDP
            callbacks_backup = callbacks.default_callbacks.copy()  # backup callbacks as check_amp() resets them
            self.amp = torch.tensor(check_amp(self.model), device=self.device)
            callbacks.default_callbacks = callbacks_backup  # restore callbacks
        if RANK > -1 and self.world_size > 1:  # DDP
            dist.broadcast(self.amp.int(), src=0)  # broadcast from rank 0 to all other ranks; gloo errors with boolean
        self.amp = bool(self.amp)  # as boolean
        self.scaler = (
            torch.amp.GradScaler("cuda", enabled=self.amp) if TORCH_2_4 else torch.cuda.amp.GradScaler(enabled=self.amp)
        )
        if self.world_size > 1:
            self.model = nn.parallel.DistributedDataParallel(self.model, device_ids=[RANK], find_unused_parameters=True)

        # Check imgsz
        gs = max(int(self.model.stride.max() if hasattr(self.model, "stride") else 32), 32)  # grid size (max stride)
        self.args.imgsz = check_imgsz(self.args.imgsz, stride=gs, floor=gs, max_dim=1)
        self.stride = gs  # for multiscale training

        # Batch size
        if self.batch_size < 1 and RANK == -1:  # single-GPU only, estimate best batch size
            self.args.batch = self.batch_size = self.auto_batch()

        # Dataloaders
        batch_size = self.batch_size // max(self.world_size, 1)
        self.train_loader = self.get_dataloader(
            self.data["train"], batch_size=batch_size, rank=LOCAL_RANK, mode="train"
        )
        # Note: When training DOTA dataset, double batch size could get OOM on images with >2000 objects.
        self.test_loader = self.get_dataloader(
            self.data.get("val") or self.data.get("test"),
            batch_size=batch_size if self.args.task == "obb" else batch_size * 2,
            rank=LOCAL_RANK,
            mode="val",
        )
        self.validator = self.get_validator()
        self.ema = ModelEMA(self.model)
        if RANK in {-1, 0}:
            metric_keys = self.validator.metrics.keys + self.label_loss_items(prefix="val")
            self.metrics = dict(zip(metric_keys, [0] * len(metric_keys)))
            if self.args.plots:
                self.plot_training_labels()

        # Optimizer
        self.accumulate = max(round(self.args.nbs / self.batch_size), 1)  # accumulate loss before optimizing
        weight_decay = self.args.weight_decay * self.batch_size * self.accumulate / self.args.nbs  # scale weight_decay
        iterations = math.ceil(len(self.train_loader.dataset) / max(self.batch_size, self.args.nbs)) * self.epochs
        self.optimizer = self.build_optimizer(
            model=self.model,
            name=self.args.optimizer,
            lr=self.args.lr0,
            momentum=self.args.momentum,
            decay=weight_decay,
            iterations=iterations,
        )
        # Scheduler
        self._setup_scheduler()
        self.stopper, self.stop = EarlyStopping(patience=self.args.patience), False
        self.resume_training(ckpt)
        self.scheduler.last_epoch = self.start_epoch - 1  # do not move
        self.run_callbacks("on_pretrain_routine_end")

    def _do_train(self):
        """ä½¿ç”¨æŒ‡å®šçš„ world size è®­ç»ƒæ¨¡å‹ã€‚"""
        if self.world_size > 1:
            self._setup_ddp()
        self._setup_train()

        nb = len(self.train_loader)  # number of batches
        nw = max(round(self.args.warmup_epochs * nb), 100) if self.args.warmup_epochs > 0 else -1  # warmup iterations
        last_opt_step = -1
        self.epoch_time = None
        self.epoch_time_start = time.time()
        self.train_time_start = time.time()
        self.run_callbacks("on_train_start")
        LOGGER.info(
            f"Image sizes {self.args.imgsz} train, {self.args.imgsz} val\n"
            f"Using {self.train_loader.num_workers * (self.world_size or 1)} dataloader workers\n"
            f"Logging results to {colorstr('bold', self.save_dir)}\n"
            f"Starting training for " + (f"{self.args.time} hours..." if self.args.time else f"{self.epochs} epochs...")
        )
        if self.args.close_mosaic:
            base_idx = (self.epochs - self.args.close_mosaic) * nb
            self.plot_idx.extend([base_idx, base_idx + 1, base_idx + 2])
        epoch = self.start_epoch
        self.optimizer.zero_grad()  # zero any resumed gradients to ensure stability on train start
        while True:
            self.epoch = epoch
            self.run_callbacks("on_train_epoch_start")
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")  # suppress 'Detected lr_scheduler.step() before optimizer.step()'
                self.scheduler.step()

            self._model_train()
            if RANK != -1:
                self.train_loader.sampler.set_epoch(epoch)
            pbar = enumerate(self.train_loader)
            # Update dataloader attributes (optional)
            if epoch == (self.epochs - self.args.close_mosaic):
                self._close_dataloader_mosaic()
                self.train_loader.reset()

            if RANK in {-1, 0}:
                LOGGER.info(self.progress_string())
                pbar = TQDM(enumerate(self.train_loader), total=nb)
            self.tloss = None
            for i, batch in pbar:
                self.run_callbacks("on_train_batch_start")
                # Warmup
                ni = i + nb * epoch
                if ni <= nw:
                    xi = [0, nw]  # x interp
                    self.accumulate = max(1, int(np.interp(ni, xi, [1, self.args.nbs / self.batch_size]).round()))
                    for j, x in enumerate(self.optimizer.param_groups):
                        # Bias lr falls from 0.1 to lr0, all other lrs rise from 0.0 to lr0
                        x["lr"] = np.interp(
                            ni, xi, [self.args.warmup_bias_lr if j == 0 else 0.0, x["initial_lr"] * self.lf(epoch)]
                        )
                        if "momentum" in x:
                            x["momentum"] = np.interp(ni, xi, [self.args.warmup_momentum, self.args.momentum])

                # Forward
                with autocast(self.amp):
                    batch = self.preprocess_batch(batch)
                    if self.args.compile:
                        # Decouple inference and loss calculations for improved compile performance
                        preds = self.model(batch["img"])
                        loss, self.loss_items = unwrap_model(self.model).loss(batch, preds)
                    else:
                        loss, self.loss_items = self.model(batch)
                    self.loss = loss.sum()
                    if RANK != -1:
                        self.loss *= self.world_size
                    self.tloss = self.loss_items if self.tloss is None else (self.tloss * i + self.loss_items) / (i + 1)

                # Backward
                self.scaler.scale(self.loss).backward()
                if ni - last_opt_step >= self.accumulate:
                    self.optimizer_step()
                    last_opt_step = ni

                    # Timed stopping
                    if self.args.time:
                        self.stop = (time.time() - self.train_time_start) > (self.args.time * 3600)
                        if RANK != -1:  # if DDP training
                            broadcast_list = [self.stop if RANK == 0 else None]
                            dist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranks
                            self.stop = broadcast_list[0]
                        if self.stop:  # training time exceeded
                            break

                # Log
                if RANK in {-1, 0}:
                    loss_length = self.tloss.shape[0] if len(self.tloss.shape) else 1
                    pbar.set_description(
                        ("%11s" * 2 + "%11.4g" * (2 + loss_length))
                        % (
                            f"{epoch + 1}/{self.epochs}",
                            f"{self._get_memory():.3g}G",  # (GB) GPU memory util
                            *(self.tloss if loss_length > 1 else torch.unsqueeze(self.tloss, 0)),  # losses
                            batch["cls"].shape[0],  # batch size, i.e. 8
                            batch["img"].shape[-1],  # imgsz, i.e 640
                        )
                    )
                    self.run_callbacks("on_batch_end")
                    if self.args.plots and ni in self.plot_idx:
                        self.plot_training_samples(batch, ni)

                self.run_callbacks("on_train_batch_end")

            self.lr = {f"lr/pg{ir}": x["lr"] for ir, x in enumerate(self.optimizer.param_groups)}  # for loggers

            self.run_callbacks("on_train_epoch_end")
            if RANK in {-1, 0}:
                self.ema.update_attr(self.model, include=["yaml", "nc", "args", "names", "stride", "class_weights"])

            # Validation
            final_epoch = epoch + 1 >= self.epochs
            if self.args.val or final_epoch or self.stopper.possible_stop or self.stop:
                self._clear_memory(threshold=0.5)  # prevent VRAM spike
                self.metrics, self.fitness = self.validate()

            # NaN recovery
            if self._handle_nan_recovery(epoch):
                continue

            self.nan_recovery_attempts = 0
            if RANK in {-1, 0}:
                self.save_metrics(metrics={**self.label_loss_items(self.tloss), **self.metrics, **self.lr})
                self.stop |= self.stopper(epoch + 1, self.fitness) or final_epoch
                if self.args.time:
                    self.stop |= (time.time() - self.train_time_start) > (self.args.time * 3600)

                # Save model
                if self.args.save or final_epoch:
                    self.save_model()
                    self.run_callbacks("on_model_save")

            # Scheduler
            t = time.time()
            self.epoch_time = t - self.epoch_time_start
            self.epoch_time_start = t
            if self.args.time:
                mean_epoch_time = (t - self.train_time_start) / (epoch - self.start_epoch + 1)
                self.epochs = self.args.epochs = math.ceil(self.args.time * 3600 / mean_epoch_time)
                self._setup_scheduler()
                self.scheduler.last_epoch = self.epoch  # do not move
                self.stop |= epoch >= self.epochs  # stop if exceeded epochs
            self.run_callbacks("on_fit_epoch_end")
            self._clear_memory(0.5)  # clear if memory utilization > 50%

            # Early Stopping
            if RANK != -1:  # if DDP training
                broadcast_list = [self.stop if RANK == 0 else None]
                dist.broadcast_object_list(broadcast_list, 0)  # broadcast 'stop' to all ranks
                self.stop = broadcast_list[0]
            if self.stop:
                break  # must break all DDP ranks
            epoch += 1

        seconds = time.time() - self.train_time_start
        LOGGER.info(f"\n{epoch - self.start_epoch + 1} epochs completed in {seconds / 3600:.3f} hours.")
        # Do final val with best.pt
        self.final_eval()
        if RANK in {-1, 0}:
            if self.args.plots:
                self.plot_metrics()
            self.run_callbacks("on_train_end")
        self._clear_memory()
        unset_deterministic()
        self.run_callbacks("teardown")

    def auto_batch(self, max_num_obj=0):
        """åŸºäºæ¨¡å‹å’Œè®¾å¤‡å†…å­˜çº¦æŸè®¡ç®—æœ€ä¼˜æ‰¹æ¬¡å¤§å°ã€‚"""
        return check_train_batch_size(
            model=self.model,
            imgsz=self.args.imgsz,
            amp=self.amp,
            batch=self.batch_size,
            max_num_obj=max_num_obj,
        )  # returns batch size

    def _get_memory(self, fraction=False):
        """è·å–åŠ é€Ÿå™¨å†…å­˜ä½¿ç”¨é‡,ä»¥ GB æˆ–æ€»å†…å­˜çš„åˆ†æ•°å½¢å¼è¿”å›ã€‚"""
        memory, total = 0, 0
        if self.device.type == "mps":
            memory = torch.mps.driver_allocated_memory()
            if fraction:
                return __import__("psutil").virtual_memory().percent / 100
        elif self.device.type != "cpu":
            memory = torch.cuda.memory_reserved()
            if fraction:
                total = torch.cuda.get_device_properties(self.device).total_memory
        return ((memory / total) if total > 0 else 0) if fraction else (memory / 2**30)

    def _clear_memory(self, threshold: float | None = None):
        """é€šè¿‡è°ƒç”¨åƒåœ¾å›æ”¶å™¨å’Œæ¸…ç©ºç¼“å­˜æ¥æ¸…ç†åŠ é€Ÿå™¨å†…å­˜ã€‚"""
        if threshold:
            assert 0 <= threshold <= 1, "Threshold must be between 0 and 1."
            if self._get_memory(fraction=True) <= threshold:
                return
        gc.collect()
        if self.device.type == "mps":
            torch.mps.empty_cache()
        elif self.device.type == "cpu":
            return
        else:
            torch.cuda.empty_cache()

    def read_results_csv(self):
        """ä½¿ç”¨ polars å°† results.csv è¯»å–åˆ°å­—å…¸ä¸­ã€‚"""
        import polars as pl  # scope for faster 'import ultralytics'

        try:
            return pl.read_csv(self.csv, infer_schema_length=None).to_dict(as_series=False)
        except Exception:
            return {}

    def _model_train(self):
        """å°†æ¨¡å‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼ã€‚"""
        self.model.train()
        # Freeze BN stat
        for n, m in self.model.named_modules():
            if any(filter(lambda f: f in n, self.freeze_layer_names)) and isinstance(m, nn.BatchNorm2d):
                m.eval()

    def save_model(self):
        """ä¿å­˜å¸¦æœ‰é¢å¤–å…ƒæ•°æ®çš„æ¨¡å‹è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚"""
        import io

        # Serialize ckpt to a byte buffer once (faster than repeated torch.save() calls)
        buffer = io.BytesIO()
        torch.save(
            {
                "epoch": self.epoch,
                "best_fitness": self.best_fitness,
                "model": None,  # resume and final checkpoints derive from EMA
                "ema": deepcopy(unwrap_model(self.ema.ema)).half(),
                "updates": self.ema.updates,
                "optimizer": convert_optimizer_state_dict_to_fp16(deepcopy(self.optimizer.state_dict())),
                "scaler": self.scaler.state_dict(),
                "train_args": vars(self.args),  # save as dict
                "train_metrics": {**self.metrics, **{"fitness": self.fitness}},
                "train_results": self.read_results_csv(),
                "date": datetime.now().isoformat(),
                "version": __version__,
                "git": {
                    "root": str(GIT.root),
                    "branch": GIT.branch,
                    "commit": GIT.commit,
                    "origin": GIT.origin,
                },
                "license": "AGPL-3.0 (https://ultralytics.com/license)",
                "docs": "https://docs.ultralytics.com",
            },
            buffer,
        )
        serialized_ckpt = buffer.getvalue()  # get the serialized content to save

        # Save checkpoints
        self.wdir.mkdir(parents=True, exist_ok=True)  # ensure weights directory exists
        self.last.write_bytes(serialized_ckpt)  # save last.pt
        if self.best_fitness == self.fitness:
            self.best.write_bytes(serialized_ckpt)  # save best.pt
        if (self.save_period > 0) and (self.epoch % self.save_period == 0):
            (self.wdir / f"epoch{self.epoch}.pt").write_bytes(serialized_ckpt)  # save epoch, i.e. 'epoch3.pt'

    def get_dataset(self):
        """ä»æ•°æ®å­—å…¸è·å–è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€‚

        è¿”å›:
            (dict): åŒ…å«è®­ç»ƒ/éªŒè¯/æµ‹è¯•æ•°æ®é›†å’Œç±»åˆ«åç§°çš„å­—å…¸ã€‚
        """
        try:
            if self.args.task == "classify":
                data = check_cls_dataset(self.args.data)
            elif str(self.args.data).rsplit(".", 1)[-1] == "ndjson":
                # Convert NDJSON to YOLO format
                import asyncio

                from ultralytics.data.converter import convert_ndjson_to_yolo

                yaml_path = asyncio.run(convert_ndjson_to_yolo(self.args.data))
                self.args.data = str(yaml_path)
                data = check_det_dataset(self.args.data)
            elif str(self.args.data).rsplit(".", 1)[-1] in {"yaml", "yml"} or self.args.task in {
                "detect",
                "segment",
                "pose",
                "obb",
            }:
                data = check_det_dataset(self.args.data)
                if "yaml_file" in data:
                    self.args.data = data["yaml_file"]  # for validating 'yolo train data=url.zip' usage
        except Exception as e:
            raise RuntimeError(emojis(f"Dataset '{clean_url(self.args.data)}' error âŒ {e}")) from e
        if self.args.single_cls:
            LOGGER.info("Overriding class names with single class.")
            data["names"] = {0: "item"}
            data["nc"] = 1
        return data

    def setup_model(self):
        """ä¸ºä»»ä½•ä»»åŠ¡åŠ è½½ã€åˆ›å»ºæˆ–ä¸‹è½½æ¨¡å‹ã€‚

        è¿”å›:
            (dict): ç”¨äºæ¢å¤è®­ç»ƒçš„å¯é€‰æ£€æŸ¥ç‚¹ã€‚
        """
        if isinstance(self.model, torch.nn.Module):  # if model is loaded beforehand. No setup needed
            return

        cfg, weights = self.model, None
        ckpt = None
        if str(self.model).endswith(".pt"):
            weights, ckpt = load_checkpoint(self.model)
            cfg = weights.yaml
        elif isinstance(self.args.pretrained, (str, Path)):
            weights, _ = load_checkpoint(self.args.pretrained)
        self.model = self.get_model(cfg=cfg, weights=weights, verbose=RANK == -1)  # calls Model(cfg, weights)
        return ckpt

    def optimizer_step(self):
        """æ‰§è¡Œè®­ç»ƒä¼˜åŒ–å™¨çš„å•æ­¥æ“ä½œ,åŒ…æ‹¬æ¢¯åº¦è£å‰ªå’Œ EMA æ›´æ–°ã€‚"""
        self.scaler.unscale_(self.optimizer)  # unscale gradients
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=10.0)
        self.scaler.step(self.optimizer)
        self.scaler.update()
        self.optimizer.zero_grad()
        if self.ema:
            self.ema.update(self.model)

    def preprocess_batch(self, batch):
        """æ ¹æ®ä»»åŠ¡ç±»å‹å…è®¸è‡ªå®šä¹‰é¢„å¤„ç†æ¨¡å‹è¾“å…¥å’ŒçœŸå®æ ‡ç­¾ã€‚"""
        return batch

    def validate(self):
        """ä½¿ç”¨ self.validator åœ¨éªŒè¯é›†ä¸Šè¿è¡ŒéªŒè¯ã€‚

        è¿”å›:
            metrics (dict): éªŒè¯æŒ‡æ ‡å­—å…¸ã€‚
            fitness (float): éªŒè¯çš„é€‚åº”åº¦åˆ†æ•°ã€‚
        """
        if self.ema and self.world_size > 1:
            # Sync EMA buffers from rank 0 to all ranks
            for buffer in self.ema.ema.buffers():
                dist.broadcast(buffer, src=0)
        metrics = self.validator(self)
        if metrics is None:
            return None, None
        fitness = metrics.pop("fitness", -self.loss.detach().cpu().numpy())  # use loss as fitness measure if not found
        if not self.best_fitness or self.best_fitness < fitness:
            self.best_fitness = fitness
        return metrics, fitness

    def get_model(self, cfg=None, weights=None, verbose=True):
        """è·å–æ¨¡å‹,å¯¹äºåŠ è½½ cfg æ–‡ä»¶æŠ›å‡º NotImplementedErrorã€‚"""
        raise NotImplementedError("This task trainer doesn't support loading cfg files")

    def get_validator(self):
        """æŠ›å‡º NotImplementedError(å¿…é¡»ç”±å­ç±»å®ç°)ã€‚"""
        raise NotImplementedError("get_validator function not implemented in trainer")

    def get_dataloader(self, dataset_path, batch_size=16, rank=0, mode="train"):
        """æŠ›å‡º NotImplementedError(å¿…é¡»åœ¨å­ç±»ä¸­è¿”å› `torch.utils.data.DataLoader`)ã€‚"""
        raise NotImplementedError("get_dataloader function not implemented in trainer")

    def build_dataset(self, img_path, mode="train", batch=None):
        """æ„å»ºæ•°æ®é›†ã€‚"""
        raise NotImplementedError("build_dataset function not implemented in trainer")

    def label_loss_items(self, loss_items=None, prefix="train"):
        """è¿”å›å¸¦æ ‡ç­¾çš„è®­ç»ƒæŸå¤±é¡¹å¼ é‡çš„æŸå¤±å­—å…¸ã€‚

        æ³¨æ„:
            åˆ†ç±»ä»»åŠ¡ä¸éœ€è¦æ­¤æ–¹æ³•,ä½†åˆ†å‰²å’Œæ£€æµ‹ä»»åŠ¡éœ€è¦ã€‚
        """
        return {"loss": loss_items} if loss_items is not None else ["loss"]

    def set_model_attributes(self):
        """åœ¨è®­ç»ƒå‰è®¾ç½®æˆ–æ›´æ–°æ¨¡å‹å‚æ•°ã€‚"""
        self.model.names = self.data["names"]

    def build_targets(self, preds, targets):
        """ä¸ºè®­ç»ƒ YOLO æ¨¡å‹æ„å»ºç›®æ ‡å¼ é‡ã€‚"""
        pass

    def progress_string(self):
        """è¿”å›æè¿°è®­ç»ƒè¿›åº¦çš„å­—ç¬¦ä¸²ã€‚"""
        return ""

    # TODO: may need to put these following functions into callback
    def plot_training_samples(self, batch, ni):
        """åœ¨ YOLO è®­ç»ƒæœŸé—´ç»˜åˆ¶è®­ç»ƒæ ·æœ¬ã€‚"""
        pass

    def plot_training_labels(self):
        """ç»˜åˆ¶ YOLO æ¨¡å‹çš„è®­ç»ƒæ ‡ç­¾ã€‚"""
        pass

    def save_metrics(self, metrics):
        """å°†è®­ç»ƒæŒ‡æ ‡ä¿å­˜åˆ° CSV æ–‡ä»¶ã€‚"""
        keys, vals = list(metrics.keys()), list(metrics.values())
        n = len(metrics) + 2  # number of cols
        t = time.time() - self.train_time_start
        self.csv.parent.mkdir(parents=True, exist_ok=True)  # ensure parent directory exists
        s = "" if self.csv.exists() else ("%s," * n % ("epoch", "time", *keys)).rstrip(",") + "\n"
        with open(self.csv, "a", encoding="utf-8") as f:
            f.write(s + ("%.6g," * n % (self.epoch + 1, t, *vals)).rstrip(",") + "\n")

    def plot_metrics(self):
        """ä» CSV æ–‡ä»¶ç»˜åˆ¶æŒ‡æ ‡ã€‚"""
        plot_results(file=self.csv, on_plot=self.on_plot)  # save results.png

    def on_plot(self, name, data=None):
        """æ³¨å†Œç»˜å›¾(ä¾‹å¦‚åœ¨å›è°ƒä¸­ä½¿ç”¨)ã€‚"""
        path = Path(name)
        self.plots[path] = {"data": data, "timestamp": time.time()}

    def final_eval(self):
        """å¯¹ç›®æ ‡æ£€æµ‹ YOLO æ¨¡å‹æ‰§è¡Œæœ€ç»ˆè¯„ä¼°å’ŒéªŒè¯ã€‚"""
        model = self.best if self.best.exists() else None
        with torch_distributed_zero_first(LOCAL_RANK):  # strip only on GPU 0; other GPUs should wait
            if RANK in {-1, 0}:
                ckpt = strip_optimizer(self.last) if self.last.exists() else {}
                if model:
                    # update best.pt train_metrics from last.pt
                    strip_optimizer(self.best, updates={"train_results": ckpt.get("train_results")})
        if model:
            LOGGER.info(f"\nValidating {model}...")
            self.validator.args.plots = self.args.plots
            self.validator.args.compile = False  # disable final val compile as too slow
            self.metrics = self.validator(model=model)
            self.metrics.pop("fitness", None)
            self.run_callbacks("on_fit_epoch_end")

    def check_resume(self, overrides):
        """æ£€æŸ¥æ¢å¤æ£€æŸ¥ç‚¹æ˜¯å¦å­˜åœ¨å¹¶ç›¸åº”æ›´æ–°å‚æ•°ã€‚"""
        resume = self.args.resume
        if resume:
            try:
                exists = isinstance(resume, (str, Path)) and Path(resume).exists()
                last = Path(check_file(resume) if exists else get_latest_run())

                # Check that resume data YAML exists, otherwise strip to force re-download of dataset
                ckpt_args = load_checkpoint(last)[0].args
                if not isinstance(ckpt_args["data"], dict) and not Path(ckpt_args["data"]).exists():
                    ckpt_args["data"] = self.args.data

                resume = True
                self.args = get_cfg(ckpt_args)
                self.args.model = self.args.resume = str(last)  # reinstate model
                for k in (
                    "imgsz",
                    "batch",
                    "device",
                    "close_mosaic",
                    "augmentations",
                    "save_period",
                    "workers",
                    "cache",
                    "patience",
                    "time",
                    "freeze",
                    "val",
                    "plots",
                ):  # allow arg updates to reduce memory or update device on resume
                    if k in overrides:
                        setattr(self.args, k, overrides[k])

                # Handle augmentations parameter for resume: check if user provided custom augmentations
                if ckpt_args.get("augmentations") is not None:
                    # Augmentations were saved in checkpoint as reprs but can't be restored automatically
                    LOGGER.warning(
                        "Custom Albumentations transforms were used in the original training run but are not "
                        "being restored. To preserve custom augmentations when resuming, you need to pass the "
                        "'augmentations' parameter again to get expected results. Example: \n"
                        f"model.train(resume=True, augmentations={ckpt_args['augmentations']})"
                    )

            except Exception as e:
                raise FileNotFoundError(
                    "Resume checkpoint not found. Please pass a valid checkpoint to resume from, "
                    "i.e. 'yolo train resume model=path/to/last.pt'"
                ) from e
        self.resume = resume

    def _load_checkpoint_state(self, ckpt):
        """ä»æ£€æŸ¥ç‚¹åŠ è½½ä¼˜åŒ–å™¨ã€ç¼©æ”¾å™¨ã€EMA å’Œ best_fitnessã€‚"""
        if ckpt.get("optimizer") is not None:
            self.optimizer.load_state_dict(ckpt["optimizer"])
        if ckpt.get("scaler") is not None:
            self.scaler.load_state_dict(ckpt["scaler"])
        if self.ema and ckpt.get("ema"):
            self.ema = ModelEMA(self.model)  # validation with EMA creates inference tensors that can't be updated
            self.ema.ema.load_state_dict(ckpt["ema"].float().state_dict())
            self.ema.updates = ckpt["updates"]
        self.best_fitness = ckpt.get("best_fitness", 0.0)

    def _handle_nan_recovery(self, epoch):
        """æ£€æµ‹å¹¶é€šè¿‡åŠ è½½æœ€æ–°æ£€æŸ¥ç‚¹ä» NaN/Inf æŸå¤±å’Œé€‚åº”åº¦å´©æºƒä¸­æ¢å¤ã€‚"""
        loss_nan = self.loss is not None and not self.loss.isfinite()
        fitness_nan = self.fitness is not None and not np.isfinite(self.fitness)
        fitness_collapse = self.best_fitness and self.best_fitness > 0 and self.fitness == 0
        corrupted = RANK in {-1, 0} and loss_nan and (fitness_nan or fitness_collapse)
        reason = "Loss NaN/Inf" if loss_nan else "Fitness NaN/Inf" if fitness_nan else "Fitness collapse"
        if RANK != -1:  # DDP: broadcast to all ranks
            broadcast_list = [corrupted if RANK == 0 else None]
            dist.broadcast_object_list(broadcast_list, 0)
            corrupted = broadcast_list[0]
        if not corrupted:
            return False
        if epoch == self.start_epoch or not self.last.exists():
            LOGGER.warning(f"{reason} detected but can not recover from last.pt...")
            return False  # Cannot recover on first epoch, let training continue
        self.nan_recovery_attempts += 1
        if self.nan_recovery_attempts > 3:
            raise RuntimeError(f"Training failed: NaN persisted for {self.nan_recovery_attempts} epochs")
        LOGGER.warning(f"{reason} detected (attempt {self.nan_recovery_attempts}/3), recovering from last.pt...")
        self._model_train()  # set model to train mode before loading checkpoint to avoid inference tensor errors
        _, ckpt = load_checkpoint(self.last)
        ema_state = ckpt["ema"].float().state_dict()
        if not all(torch.isfinite(v).all() for v in ema_state.values() if isinstance(v, torch.Tensor)):
            raise RuntimeError(f"Checkpoint {self.last} is corrupted with NaN/Inf weights")
        unwrap_model(self.model).load_state_dict(ema_state)  # Load EMA weights into model
        self._load_checkpoint_state(ckpt)  # Load optimizer/scaler/EMA/best_fitness
        del ckpt, ema_state
        self.scheduler.last_epoch = epoch - 1
        return True

    def resume_training(self, ckpt):
        """ä»ç»™å®šçš„ epoch å’Œæœ€ä½³é€‚åº”åº¦æ¢å¤ YOLO è®­ç»ƒã€‚"""
        if ckpt is None or not self.resume:
            return
        start_epoch = ckpt.get("epoch", -1) + 1
        assert start_epoch > 0, (
            f"{self.args.model} training to {self.epochs} epochs is finished, nothing to resume.\n"
            f"Start a new training without resuming, i.e. 'yolo train model={self.args.model}'"
        )
        LOGGER.info(f"Resuming training {self.args.model} from epoch {start_epoch + 1} to {self.epochs} total epochs")
        if self.epochs < start_epoch:
            LOGGER.info(
                f"{self.model} has been trained for {ckpt['epoch']} epochs. Fine-tuning for {self.epochs} more epochs."
            )
            self.epochs += ckpt["epoch"]  # finetune additional epochs
        self._load_checkpoint_state(ckpt)
        self.start_epoch = start_epoch
        if start_epoch > (self.epochs - self.args.close_mosaic):
            self._close_dataloader_mosaic()

    def _close_dataloader_mosaic(self):
        """æ›´æ–°æ•°æ®åŠ è½½å™¨ä»¥åœæ­¢ä½¿ç”¨ mosaic å¢å¼ºã€‚"""
        if hasattr(self.train_loader.dataset, "mosaic"):
            self.train_loader.dataset.mosaic = False
        if hasattr(self.train_loader.dataset, "close_mosaic"):
            LOGGER.info("Closing dataloader mosaic")
            self.train_loader.dataset.close_mosaic(hyp=copy(self.args))

    def build_optimizer(self, model, name="auto", lr=0.001, momentum=0.9, decay=1e-5, iterations=1e5):
        """ä¸ºç»™å®šæ¨¡å‹æ„å»ºä¼˜åŒ–å™¨ã€‚

        å‚æ•°:
            model (torch.nn.Module): è¦æ„å»ºä¼˜åŒ–å™¨çš„æ¨¡å‹ã€‚
            name (str, optional): è¦ä½¿ç”¨çš„ä¼˜åŒ–å™¨åç§°ã€‚å¦‚æœä¸º 'auto',åˆ™æ ¹æ®è¿­ä»£æ¬¡æ•°é€‰æ‹©ä¼˜åŒ–å™¨ã€‚
            lr (float, optional): ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡ã€‚
            momentum (float, optional): ä¼˜åŒ–å™¨çš„åŠ¨é‡å› å­ã€‚
            decay (float, optional): ä¼˜åŒ–å™¨çš„æƒé‡è¡°å‡ã€‚
            iterations (float, optional): è¿­ä»£æ¬¡æ•°,å¦‚æœ name ä¸º 'auto' åˆ™ç”¨äºç¡®å®šä¼˜åŒ–å™¨ã€‚

        è¿”å›:
            (torch.optim.Optimizer): æ„å»ºçš„ä¼˜åŒ–å™¨ã€‚
        """
        g = [], [], []  # optimizer parameter groups
        bn = tuple(v for k, v in nn.__dict__.items() if "Norm" in k)  # normalization layers, i.e. BatchNorm2d()
        if name == "auto":
            LOGGER.info(
                f"{colorstr('optimizer:')} 'optimizer=auto' found, "
                f"ignoring 'lr0={self.args.lr0}' and 'momentum={self.args.momentum}' and "
                f"determining best 'optimizer', 'lr0' and 'momentum' automatically... "
            )
            nc = self.data.get("nc", 10)  # number of classes
            lr_fit = round(0.002 * 5 / (4 + nc), 6)  # lr0 fit equation to 6 decimal places
            name, lr, momentum = ("SGD", 0.01, 0.9) if iterations > 10000 else ("AdamW", lr_fit, 0.9)
            self.args.warmup_bias_lr = 0.0  # no higher than 0.01 for Adam

        for module_name, module in model.named_modules():
            for param_name, param in module.named_parameters(recurse=False):
                fullname = f"{module_name}.{param_name}" if module_name else param_name
                if "bias" in fullname:  # bias (no decay)
                    g[2].append(param)
                elif isinstance(module, bn) or "logit_scale" in fullname:  # weight (no decay)
                    # ContrastiveHead and BNContrastiveHead included here with 'logit_scale'
                    g[1].append(param)
                else:  # weight (with decay)
                    g[0].append(param)

        optimizers = {"Adam", "Adamax", "AdamW", "NAdam", "RAdam", "RMSProp", "SGD", "auto"}
        name = {x.lower(): x for x in optimizers}.get(name.lower())
        if name in {"Adam", "Adamax", "AdamW", "NAdam", "RAdam"}:
            optimizer = getattr(optim, name, optim.Adam)(g[2], lr=lr, betas=(momentum, 0.999), weight_decay=0.0)
        elif name == "RMSProp":
            optimizer = optim.RMSprop(g[2], lr=lr, momentum=momentum)
        elif name == "SGD":
            optimizer = optim.SGD(g[2], lr=lr, momentum=momentum, nesterov=True)
        else:
            raise NotImplementedError(
                f"Optimizer '{name}' not found in list of available optimizers {optimizers}. "
                "Request support for addition optimizers at https://github.com/ultralytics/ultralytics."
            )

        optimizer.add_param_group({"params": g[0], "weight_decay": decay})  # add g0 with weight_decay
        optimizer.add_param_group({"params": g[1], "weight_decay": 0.0})  # add g1 (BatchNorm2d weights)
        LOGGER.info(
            f"{colorstr('optimizer:')} {type(optimizer).__name__}(lr={lr}, momentum={momentum}) with parameter groups "
            f"{len(g[1])} weight(decay=0.0), {len(g[0])} weight(decay={decay}), {len(g[2])} bias(decay=0.0)"
        )
        return optimizer
